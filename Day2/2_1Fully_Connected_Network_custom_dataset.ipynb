{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Random Seed:  9432\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from torchvision import models, transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################## configure device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # 1 is another GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "################## Set random seem for reproducibility\n",
    "manualSeed = 9432\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "plt.ion()   # interactive mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Steps for training a Neural Network are:\n",
    "\n",
    "1. Load Data by creating a dataloader\n",
    "2. Define the Network Model\n",
    "3. Training the Network\n",
    "4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load Data : Dataloaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations to be used for Data Augmentagtion, \n",
    "apply_transform = transforms.Compose([transforms.Resize(28),transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchvision.transforms operate on PIL images. In general, additiona Data Augementation methods are applied only to the training set and not the validation/test sets.\n",
    "\n",
    "See the following commonly used transforms in : https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "\n",
    "Normalize(mean, std), RandomHorizontalFlip(),RandomVerticalFlip(), RandomResizedCrop(), ColorJitter(), RandomAffine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root=os.getcwd()+'/data/',\n",
    "                                           train=True, \n",
    "                                           transform=apply_transform,\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root=os.getcwd()+'/data/',\n",
    "                                          train=False, \n",
    "                                          transform=apply_transform,\n",
    "                                          download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "\n",
    "class MNIST_train(Dataset):\n",
    "    # customized dataset\n",
    "    def __init__(self, img_path, csv_name, transforms):\n",
    "                \n",
    "        img_nm=[]\n",
    "        lbl=[]\n",
    "        # read the entire csv file and save image_name, lbls\n",
    "        with open(csv_name) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            for row in csv_reader:\n",
    "                img_nm.append(row[0])\n",
    "                lbl.append(int(float(row[1])))\n",
    "                \n",
    "        self.img_nm=img_nm\n",
    "        self.lbl=lbl\n",
    "        self.img_path=img_path # dir of images.\n",
    "        self.transform=transforms\n",
    "        \n",
    "      \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        ############# Return the sample data,gt for the input index\n",
    "        # read the input image, gt\n",
    "        # Read image as PIL\n",
    "        tmp_img = Image.open(self.img_path+self.img_nm[index])\n",
    "        # apply transform\n",
    "        if self.transform is not None:\n",
    "            tmp_img = self.transform(tmp_img)\n",
    "        \n",
    "        # convert label to tensor\n",
    "        tmp_lbl=self.lbl[index]\n",
    "        \n",
    "        tmp_lbl=torch.from_numpy(np.array(tmp_lbl))\n",
    "        tmp_lbl=tmp_lbl.long()\n",
    "         \n",
    "        return (tmp_img, tmp_lbl)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        # Compute total number of samples in the dataset and return\n",
    "        return len(self.lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=MNIST_train(os.getcwd()+'/data/MNIST_cutsom_dataloader/Train/', os.getcwd()+'/data/MNIST_cutsom_dataloader/train_part.csv', apply_transform)\n",
    "\n",
    "val_dataset=MNIST_train(os.getcwd()+'/data/MNIST_cutsom_dataloader/Train/', os.getcwd()+'/data/MNIST_cutsom_dataloader/val_part.csv', apply_transform)\n",
    "\n",
    "test_dataset=MNIST_train(os.getcwd()+'/data/MNIST_cutsom_dataloader/Test/', os.getcwd()+'/data/MNIST_cutsom_dataloader/test.csv', apply_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4f845d0240>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(f, lbl) = train_dataset[0]\n",
    "print(f.shape)\n",
    "print(lbl)\n",
    "\n",
    "fig=plt.figure(figsize=(12, 12)) \n",
    "fig.add_subplot(2, 1, 1) \n",
    "f=f.numpy()\n",
    "f=np.squeeze(f)\n",
    "plt.imshow(f, cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=32, \n",
    "                                           shuffle=True, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=32, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Defining the Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, ftr_dim1, ftr_dim2, nclasses):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.Layer1 = nn.Sequential(\n",
    "            nn.Linear(28*28, ftr_dim1),\n",
    "            nn.BatchNorm1d(num_features=ftr_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ftr_dim1, ftr_dim2),\n",
    "            nn.BatchNorm1d(num_features=ftr_dim2),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(ftr_dim2, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0],-1)# Flatten (batch_sz,1,28, 28) image to (batch_sz,28*28) \n",
    "        x = self.Layer1(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (Layer1): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
      "    (1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=400, out_features=256, bias=True)\n",
      "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "No. of learnable Network Parameters= 420538\n"
     ]
    }
   ],
   "source": [
    "# create an instance of the model\n",
    "model=NeuralNet(ftr_dim1=400, ftr_dim2=256, nclasses=10)\n",
    "model=model.to(device) # Transfer the model from CPU to GPU\n",
    "print(model) # Print the model architecture\n",
    "\n",
    "\n",
    "\n",
    "# Count no of learnable parameters in the model\n",
    "def count_parameters(model):\n",
    "    # Returns only trainable params due to the last if\n",
    "    # wouldnot work for shared parameters which will be counted multiple times\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print( \"No. of learnable Network Parameters= \"+str(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    correct = 0 # Correctly predicted \n",
    "    total = 0 # Total number of samples\n",
    "    running_loss=0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(val_loader):\n",
    "            \n",
    "            images = images.to(device) # put data into gpu\n",
    "            labels = labels.to(device)\n",
    "                    \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "            # torch.max returns  a tuple (max_value, max_idx), the mx_idx gives the class label \n",
    "            predicted = torch.max(outputs, 1)[1] \n",
    "    \n",
    "            # Compute Accuracy\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.detach() == labels).sum().item()\n",
    "\n",
    "            running_loss=running_loss+loss.item() \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    acc=correct/total\n",
    "    model.train()\n",
    "    print (\"\\n Val_acc:  {:.4f}, Val_Classification Loss: {:.4f}\"\n",
    "                   .format(acc, running_loss/(i+1) ))\n",
    "            \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain an instance of loss and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "init_lr=0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Batch [1500/1500] Train Loss: 0.2483\n",
      " Val_acc:  0.9616, Val_Classification Loss: 0.1330\n",
      "Val Performance improved, Saving checkpoint.. in best_weight_FC.pt\n",
      "Epoch [2/100], Batch [1500/1500] Train Loss: 0.1312\n",
      " Val_acc:  0.9705, Val_Classification Loss: 0.0990\n",
      "Val Performance improved, Saving checkpoint.. in best_weight_FC.pt\n",
      "Epoch [3/100], Batch [1500/1500] Train Loss: 0.1040\n",
      " Val_acc:  0.9745, Val_Classification Loss: 0.0866\n",
      "Val Performance improved, Saving checkpoint.. in best_weight_FC.pt\n",
      "Epoch [4/100], Batch [1500/1500] Train Loss: 0.0879\n",
      " Val_acc:  0.9767, Val_Classification Loss: 0.0854\n",
      "Val Performance improved, Saving checkpoint.. in best_weight_FC.pt\n",
      "Epoch [5/100], Batch [1500/1500] Train Loss: 0.0742\n",
      " Val_acc:  0.9709, Val_Classification Loss: 0.1043\n",
      "Validation metric has not improved in last 1 batch updates\n",
      "Epoch [6/100], Batch [1500/1500] Train Loss: 0.0662\n",
      " Val_acc:  0.9758, Val_Classification Loss: 0.0872\n",
      "Validation metric has not improved in last 2 batch updates\n",
      "Epoch [7/100], Batch [1500/1500] Train Loss: 0.0586\n",
      " Val_acc:  0.9775, Val_Classification Loss: 0.0894\n",
      "Val Performance improved, Saving checkpoint.. in best_weight_FC.pt\n",
      "Epoch [8/100], Batch [1500/1500] Train Loss: 0.0531\n",
      " Val_acc:  0.9776, Val_Classification Loss: 0.0915\n",
      "Val Performance improved, Saving checkpoint.. in best_weight_FC.pt\n",
      "Epoch [9/100], Batch [1500/1500] Train Loss: 0.0506\n",
      " Val_acc:  0.9749, Val_Classification Loss: 0.1025\n",
      "Validation metric has not improved in last 1 batch updates\n",
      "Epoch [10/100], Batch [1500/1500] Train Loss: 0.0427\n",
      " Val_acc:  0.9795, Val_Classification Loss: 0.0855\n",
      "Val Performance improved, Saving checkpoint.. in best_weight_FC.pt\n",
      "Epoch [11/100], Batch [1500/1500] Train Loss: 0.0409\n",
      " Val_acc:  0.9734, Val_Classification Loss: 0.1157\n",
      "Validation metric has not improved in last 1 batch updates\n",
      "Epoch [12/100], Batch [1500/1500] Train Loss: 0.0398\n",
      " Val_acc:  0.9795, Val_Classification Loss: 0.0859\n",
      "Validation metric has not improved in last 2 batch updates\n",
      "Epoch [13/100], Batch [1500/1500] Train Loss: 0.0372\n",
      " Val_acc:  0.9801, Val_Classification Loss: 0.0863\n",
      "Val Performance improved, Saving checkpoint.. in best_weight_FC.pt\n",
      "Epoch [14/100], Batch [1500/1500] Train Loss: 0.0351\n",
      " Val_acc:  0.9784, Val_Classification Loss: 0.1139\n",
      "Validation metric has not improved in last 1 batch updates\n",
      "Epoch [15/100], Batch [1500/1500] Train Loss: 0.0332\n",
      " Val_acc:  0.9788, Val_Classification Loss: 0.1082\n",
      "Validation metric has not improved in last 2 batch updates\n",
      "Epoch [16/100], Batch [1500/1500] Train Loss: 0.0314\n",
      " Val_acc:  0.9758, Val_Classification Loss: 0.1146\n",
      "Validation metric has not improved in last 3 batch updates\n",
      "Epoch [17/100], Batch [1500/1500] Train Loss: 0.0339\n",
      " Val_acc:  0.9807, Val_Classification Loss: 0.1010\n",
      "Val Performance improved, Saving checkpoint.. in best_weight_FC.pt\n",
      "Epoch [18/100], Batch [1500/1500] Train Loss: 0.0286\n",
      " Val_acc:  0.9786, Val_Classification Loss: 0.1042\n",
      "Validation metric has not improved in last 1 batch updates\n",
      "Epoch [19/100], Batch [1500/1500] Train Loss: 0.0285\n",
      " Val_acc:  0.9787, Val_Classification Loss: 0.1110\n",
      "Validation metric has not improved in last 2 batch updates\n",
      "Epoch [20/100], Batch [1500/1500] Train Loss: 0.0298\n",
      " Val_acc:  0.9788, Val_Classification Loss: 0.1049\n",
      "Validation metric has not improved in last 3 batch updates\n",
      "Epoch [21/100], Batch [1500/1500] Train Loss: 0.0278\n",
      " Val_acc:  0.9792, Val_Classification Loss: 0.1049\n",
      "Validation metric has not improved in last 4 batch updates\n",
      "Epoch [22/100], Batch [1500/1500] Train Loss: 0.0244\n",
      " Val_acc:  0.9812, Val_Classification Loss: 0.1060\n",
      "Val Performance improved, Saving checkpoint.. in best_weight_FC.pt\n",
      "Epoch [23/100], Batch [1500/1500] Train Loss: 0.0270\n",
      " Val_acc:  0.9814, Val_Classification Loss: 0.1089\n",
      "Val Performance improved, Saving checkpoint.. in best_weight_FC.pt\n",
      "Epoch [24/100], Batch [1500/1500] Train Loss: 0.0278\n",
      " Val_acc:  0.9785, Val_Classification Loss: 0.1106\n",
      "Validation metric has not improved in last 1 batch updates\n",
      "Epoch [25/100], Batch [1500/1500] Train Loss: 0.0211\n",
      " Val_acc:  0.9790, Val_Classification Loss: 0.1184\n",
      "Validation metric has not improved in last 2 batch updates\n",
      "Epoch [26/100], Batch [1500/1500] Train Loss: 0.0286\n",
      " Val_acc:  0.9815, Val_Classification Loss: 0.1044\n",
      "Val Performance improved, Saving checkpoint.. in best_weight_FC.pt\n",
      "Epoch [27/100], Batch [1500/1500] Train Loss: 0.0227\n",
      " Val_acc:  0.9796, Val_Classification Loss: 0.1155\n",
      "Validation metric has not improved in last 1 batch updates\n",
      "Epoch [28/100], Batch [1500/1500] Train Loss: 0.0225\n",
      " Val_acc:  0.9810, Val_Classification Loss: 0.1020\n",
      "Validation metric has not improved in last 2 batch updates\n",
      "Epoch [29/100], Batch [1500/1500] Train Loss: 0.0208\n",
      " Val_acc:  0.9801, Val_Classification Loss: 0.1155\n",
      "Validation metric has not improved in last 3 batch updates\n",
      "Epoch [30/100], Batch [1500/1500] Train Loss: 0.0204\n",
      " Val_acc:  0.9805, Val_Classification Loss: 0.1141\n",
      "Validation metric has not improved in last 4 batch updates\n",
      "Epoch [31/100], Batch [1500/1500] Train Loss: 0.0221\n",
      " Val_acc:  0.9810, Val_Classification Loss: 0.1165\n",
      "Validation metric has not improved in last 5 batch updates\n",
      "Epoch [32/100], Batch [1500/1500] Train Loss: 0.0230\n",
      " Val_acc:  0.9808, Val_Classification Loss: 0.1225\n",
      "Validation metric has not improved in last 6 batch updates\n",
      "Epoch [33/100], Batch [1500/1500] Train Loss: 0.0200\n",
      " Val_acc:  0.9796, Val_Classification Loss: 0.1217\n",
      "Validation metric has not improved in last 7 batch updates\n",
      "Epoch [34/100], Batch [1500/1500] Train Loss: 0.0224\n",
      " Val_acc:  0.9814, Val_Classification Loss: 0.1119\n",
      "Validation metric has not improved in last 8 batch updates\n",
      "Epoch [35/100], Batch [1500/1500] Train Loss: 0.0195\n",
      " Val_acc:  0.9795, Val_Classification Loss: 0.1225\n",
      "Validation metric has not improved in last 9 batch updates\n",
      "Epoch [36/100], Batch [1500/1500] Train Loss: 0.0192\n",
      " Val_acc:  0.9793, Val_Classification Loss: 0.1264\n",
      "Validation metric has not improved in last 10 batch updates\n",
      "Early Stopping !\n"
     ]
    }
   ],
   "source": [
    "############## TRAINING ###########\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "             \n",
    "        \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader) # Number of batch updates in each epoch\n",
    "curr_lr = init_lr\n",
    "num_epochs=100 # maximum number of epochs for training\n",
    "\n",
    "ptnc_cnt=0 # for Early stopping\n",
    "patience=10 # Stop training if the val accuracy doesnot improve for \"patience\" epochs\n",
    "max_metric=0 # best val accuracy encountered so far\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_loss=0 # approximates the training loss by computing running average\n",
    "    model.train() # Certain layers (for eg., BatchNorm and Dropout have different behaviours during training and inference)\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device) # put data into gpu\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() #  remove previous gradients accumulated in optmizer\n",
    "        loss.backward() # automatic gradient computation for each learnable parameter\n",
    "        optimizer.step() # Update the learnable parameters\n",
    "        \n",
    "        \n",
    "        # Display Training Loss after every 25 batch of updates for current epoch\n",
    "        running_loss=running_loss+loss.item()\n",
    "        if (i+1) % 25 == 0:\n",
    "            print (\"Epoch [{}/{}], Batch [{}/{}] Train Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, running_loss/(i+1)), end =\"\\r\")\n",
    "            \n",
    "            \n",
    "    ######### End of An Epoch ##################\n",
    "    \n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        curr_lr /= 10\n",
    "        update_lr(optimizer, curr_lr)\n",
    "        \n",
    "    # Monitor validation Loss\n",
    "    metric=validate(val_loader, model, criterion)\n",
    "    \n",
    "    # Checkpoint and Early Stopping \n",
    "    if metric>max_metric:\n",
    "        nm='best_weight_FC.pt'\n",
    "        print(\"Val Performance improved, Saving checkpoint.. in \"+nm)\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, nm)\n",
    "        ptnc_cnt=0\n",
    "        max_metric=metric\n",
    "            \n",
    "    else:\n",
    "        ptnc_cnt=ptnc_cnt+1\n",
    "        print('Validation metric has not improved in last '+str(ptnc_cnt)+' batch updates')\n",
    "        if ptnc_cnt==patience:\n",
    "            print(\"Early Stopping !\")\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Val_acc:  0.9809, Val_Classification Loss: 0.0951\n"
     ]
    }
   ],
   "source": [
    "##### Training is complete, now load the best training weight from the checkpoint\n",
    "checkpoint = torch.load('best_weight_FC.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "del checkpoint\n",
    "\n",
    "metric=validate(test_loader, model, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
