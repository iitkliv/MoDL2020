{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Random Seed:  9432\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "#from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from torchvision import models, transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################## configure device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # 1 is another GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "################## Set random seem for reproducibility\n",
    "manualSeed = 9432\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "plt.ion()   # interactive mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Steps for training a Neural Network are:\n",
    "\n",
    "1. Load Data by creating a dataloader\n",
    "2. Define the Network Model\n",
    "3. Training the Network\n",
    "4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load Data : Dataloaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations to be used for Data Augmentagtion, \n",
    "apply_transform = transforms.Compose([transforms.Resize(32),transforms.ToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "\n",
    "class MNIST_train(Dataset):\n",
    "    # customized dataset\n",
    "    def __init__(self, img_path, csv_name, transforms):\n",
    "        \n",
    "       \n",
    "        \n",
    "        img_nm=[]\n",
    "        lbl=[]\n",
    "        # read the entire csv file and save image_name, lbls\n",
    "        with open(csv_name) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            for row in csv_reader:\n",
    "                img_nm.append(row[0])\n",
    "                lbl.append(int(float(row[1])))\n",
    "                \n",
    "               \n",
    "        \n",
    "        self.img_nm=img_nm\n",
    "        self.lbl=lbl\n",
    "        self.img_path=img_path # dir of images.\n",
    "        self.transform=transforms\n",
    "        \n",
    "      \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        ############# Return the sample data,gt for the input index\n",
    "        # read the input image, gt\n",
    "        # Read image as PIL\n",
    "        tmp_img = Image.open(self.img_path+self.img_nm[index])\n",
    "        # apply transform\n",
    "        if self.transform is not None:\n",
    "            tmp_img = self.transform(tmp_img)\n",
    "        \n",
    "        # convert label to tensor\n",
    "        tmp_lbl=self.lbl[index]\n",
    "        \n",
    "        tmp_lbl=torch.from_numpy(np.array(tmp_lbl))\n",
    "        tmp_lbl=tmp_lbl.long()\n",
    "        \n",
    "        \n",
    "        \n",
    "        return (tmp_img, tmp_lbl)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        # Compute total number of samples in the dataset and return\n",
    "        return len(self.lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset=MNIST_train(os.getcwd()+'/data/MNIST_cutsom_dataloader/Train/', os.getcwd()+'/data/MNIST_cutsom_dataloader/train_part.csv', apply_transform)\n",
    "\n",
    "val_dataset=MNIST_train(os.getcwd()+'/data/MNIST_cutsom_dataloader/Train/', os.getcwd()+'/data/MNIST_cutsom_dataloader/val_part.csv', apply_transform)\n",
    "\n",
    "test_dataset=MNIST_train(os.getcwd()+'/data/MNIST_cutsom_dataloader/Test/', os.getcwd()+'/data/MNIST_cutsom_dataloader/test.csv', apply_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32])\n",
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f75c6a311d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(f, lbl) = train_dataset[0]\n",
    "print(f.shape)\n",
    "print(lbl)\n",
    "\n",
    "fig=plt.figure(figsize=(12, 12)) \n",
    "fig.add_subplot(2, 1, 1) \n",
    "f=f.numpy()\n",
    "f=np.squeeze(f)\n",
    "plt.imshow(f, cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=32, \n",
    "                                           shuffle=True, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=32, shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Defining the Network Model\n",
    "![title](lenet1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Modified LeNet with Batch Norm Added\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "                                    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, \n",
    "                                              stride=1, padding=0, dilation=1, groups=1, bias=False),\n",
    "                                    nn.BatchNorm2d(6)\n",
    "                                    )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "                                    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, \n",
    "                                              stride=1, padding=0, dilation=1, groups=1, bias=False),\n",
    "                                    nn.BatchNorm2d(16)\n",
    "                                    )\n",
    "           \n",
    "            \n",
    "        self.fc1=nn.Sequential(\n",
    "                                    nn.Linear( in_features=16*5*5, out_features=120, bias=False),\n",
    "                                    nn.BatchNorm1d(120)\n",
    "                                    )\n",
    "        \n",
    "        self.fc2=nn.Sequential(\n",
    "                                    nn.Linear( in_features=120, out_features=84, bias=False),\n",
    "                                    nn.BatchNorm1d(84)\n",
    "                                    )\n",
    "                                    \n",
    "        \n",
    "       \n",
    "        self.fc3=nn.Linear( in_features=84, out_features=10, bias=True)\n",
    "                                    \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        \n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Implement a CNN model of form: Conv+Maxpool->Conv+Maxpool with a variable \"depth\" number of blocks.\n",
    "# The number of Convolution filters are doubled after each block. \n",
    "# The feature dimensionality at the end of the Conv layers must be fixed, independent of the input image size\n",
    "\n",
    "### The Challenge is to write a code which generates the model in an iterative manner as depth is variable.\n",
    "### Global Avg Pooling ensures that the output feature dim is independent of input image size\n",
    "    # Other advantages: a) Low dimensionality of the FC layers   b) Explainability with CAM.\n",
    "\n",
    "\n",
    "#This concept is often employed for constructing CNN.\n",
    "#See for eg., DenseNet implementation: https://github.com/bamos/densenet.pytorch/blob/master/densenet.py line 99 onwards\n",
    "\n",
    "class my_Conv(nn.Module):\n",
    "    def __init__(self, inp_chnls, out_chnls):\n",
    "        super(my_Conv, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "                                    nn.Conv2d(in_channels=inp_chnls, out_channels=out_chnls, kernel_size=3, \n",
    "                                              stride=1, padding=1, dilation=1, groups=1, bias=False),\n",
    "                                    nn.BatchNorm2d(out_chnls),\n",
    "                                    nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                                    )\n",
    "       \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class my_CNN(nn.Module):\n",
    "    def __init__(self, base_chnls, depth):\n",
    "        super(my_CNN, self).__init__()\n",
    "        \n",
    "        layer_list=[]\n",
    "        layer_list.append(my_Conv(1, base_chnls))\n",
    "        for i in range(0, depth-1):\n",
    "            layer_list.append(my_Conv(base_chnls, 2*base_chnls))\n",
    "            base_chnls=base_chnls*2\n",
    "            \n",
    "        self.layers=nn.Sequential(*layer_list)\n",
    "        \n",
    "        self.fc=nn.Linear( in_features=base_chnls, out_features=10, bias=True)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        ftr = self.layers(x)\n",
    "        ftr =  F.adaptive_avg_pool2d(ftr, (1,1))\n",
    "        ftr = ftr.view(ftr.size(0), -1)\n",
    "        out=self.fc(ftr)\n",
    "        return out   \n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Another way to code using ModuleList: see for example: \n",
    "# https://github.com/jvanvugt/pytorch-unet/blob/master/unet.py\n",
    "# line 48-53 , 66-67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_CNN(\n",
      "  (layers): Sequential(\n",
      "    (0): my_Conv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (1): my_Conv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(2, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (2): my_Conv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (3): my_Conv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (4): my_Conv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "No. of learnable Network Parameters= 6592\n"
     ]
    }
   ],
   "source": [
    "#model=LeNet() # 1. Simple CNN: LeNet\n",
    "\n",
    "model=my_CNN(base_chnls=2, depth=5)\n",
    "\n",
    "model=model.to(device) # Transfer the model from CPU to GPU\n",
    "print(model) # Print the model architecture\n",
    "\n",
    "\n",
    "\n",
    "# Count no of learnable parameters in the model\n",
    "def count_parameters(model):\n",
    "    # Returns only trainable params due to the last if\n",
    "    # wouldnot work for shared parameters which will be counted multiple times\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print( \"No. of learnable Network Parameters= \"+str(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    correct = 0 # Correctly predicted \n",
    "    total = 0 # Total number of samples\n",
    "    running_loss=0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(val_loader):\n",
    "            \n",
    "            images = images.to(device) # put data into gpu\n",
    "            labels = labels.to(device)\n",
    "                    \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "            # torch.max returns  a tuple (max_value, max_idx), the mx_idx gives the class label \n",
    "            predicted = torch.max(outputs, 1)[1] \n",
    "    \n",
    "            # Compute Accuracy\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.detach() == labels).sum().item()\n",
    "\n",
    "            running_loss=running_loss+loss.item() \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    acc=correct/total\n",
    "    model.train()\n",
    "    print (\"\\n Val_acc:  {:.4f}, Val_Classification Loss: {:.4f}\"\n",
    "                   .format(acc, running_loss/(i+1) ))\n",
    "            \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain an instance of loss and optimizer \n",
    "criterion = nn.CrossEntropyLoss() # This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class\n",
    "\n",
    "init_lr=0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Batch [1500/1500] Train Loss: 0.2006\n",
      " Val_acc:  0.9585, Val_Classification Loss: 0.1293\n",
      "Val Performance improved, Saving checkpoint.. in best_wt_CNN.pt\n",
      "Epoch [2/100], Batch [1500/1500] Train Loss: 0.1032\n",
      " Val_acc:  0.9712, Val_Classification Loss: 0.0934\n",
      "Val Performance improved, Saving checkpoint.. in best_wt_CNN.pt\n",
      "Epoch [3/100], Batch [225/1500] Train Loss: 0.0783\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ddbfe8c825bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Certain layers (for eg., BatchNorm and Dropout have different behaviours during training and inference)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# put data into gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_batch\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstorage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mfd_id\u001b[0;34m(fd)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;31m# this doesn't work with shared memory handles, which is why we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# support the \"file_descriptor\" sharing method on that platform.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_ino\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############## TRAINING ###########\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "             \n",
    "        \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader) # Number of batch updates in each epoch\n",
    "curr_lr = init_lr\n",
    "num_epochs=100 # maximum number of epochs for training\n",
    "\n",
    "ptnc_cnt=0 # for Early stopping\n",
    "patience=10 # Stop training if the val accuracy doesnot improve for \"patience\" epochs\n",
    "max_metric=0 # best val accuracy encountered so far\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_loss=0 # approximates the training loss by computing running average\n",
    "    model.train() # Certain layers (for eg., BatchNorm and Dropout have different behaviours during training and inference)\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device) # put data into gpu\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # # remove previous gradients\n",
    "        loss.backward() # backpropagation through automatic gradient computation\n",
    "        \n",
    "        optimizer.step() # Update the optimizer parameters\n",
    "        \n",
    "        \n",
    "        # Display Training Loss after every 25 batch of updates for current epoch\n",
    "        running_loss=running_loss+loss.item()\n",
    "        if (i+1) % 25 == 0:\n",
    "            print (\"Epoch [{}/{}], Batch [{}/{}] Train Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, running_loss/(i+1)), end =\"\\r\")\n",
    "            \n",
    "            \n",
    "    ######### End of An Epoch ##################\n",
    "    \n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        curr_lr /= 10\n",
    "        update_lr(optimizer, curr_lr)\n",
    "        \n",
    "    # Monitor validation Loss\n",
    "    metric=validate(val_loader, model, criterion)\n",
    "    \n",
    "    # Checkpoint and Early Stopping \n",
    "    if metric>max_metric:\n",
    "        nm='best_wt_CNN.pt'\n",
    "        print(\"Val Performance improved, Saving checkpoint.. in \"+nm)\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, nm)\n",
    "        ptnc_cnt=0\n",
    "        max_metric=metric\n",
    "            \n",
    "    else:\n",
    "        ptnc_cnt=ptnc_cnt+1\n",
    "        print('Validation metric has not improved in last '+str(ptnc_cnt)+' batch updates')\n",
    "        if ptnc_cnt==patience:\n",
    "            print(\"Early Stopping !\")\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Val_acc:  0.9753, Val_Classification Loss: 0.0776\n"
     ]
    }
   ],
   "source": [
    "##### Training is complete, now load the best training weight from the checkpoint\n",
    "checkpoint = torch.load(nm)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "del checkpoint\n",
    "\n",
    "metric=validate(test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
