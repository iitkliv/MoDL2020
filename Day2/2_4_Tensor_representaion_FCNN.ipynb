{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoDL 2020\n",
    "# Tensor Representation of Fully Connected Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x): #sigmoid activation function\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at the representation of a fully connected network with no hidden layers. We'd have a weight matrix. Below is a neural net with three input and two output layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[0.4,0.3,0.3],[0.2,0.5,0.3]]) #weight matrix\n",
    "x = np.array([[0.6],[0.4],[0.5]]).reshape(3,1) #input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = W@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6248064744684293, 0.6153837563911821]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [sig(float(p[i])) for i in range(len(p))] \n",
    "y #output values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at the representation of a fully connected network with two hidden layers. The input layer has 10 nodes, the hidden layers each have 20 nodes, and the output layer has three nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(-1,1,10).reshape(10,1) #input\n",
    "\n",
    "W1 = np.random.uniform(0,1,200).reshape(20,10) #weights corresponding to first hidden layer\n",
    "\n",
    "W2 = np.random.uniform(0,1,400).reshape(20,20) #weights corresponding to second hidden layer\n",
    "\n",
    "W3 = np.random.uniform(0,1,60).reshape(3,20) #weights corresponding to the output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(arr): #hyperbolic tangent activation function whose input is an array\n",
    "    return (1-np.exp(-2*arr))/(1+np.exp(-2*arr))\n",
    "\n",
    "def ReLu(arr): #ReLu activation function whsoe input is an array\n",
    "    out = np.array([np.max([0,arr[i][0]]) for i in range(len(arr))]).reshape(len(arr),1)\n",
    "    return out\n",
    "    \n",
    "\n",
    "def SoftMax(arr): #SoftMax function whose input is an array\n",
    "    return np.exp(arr) / float(sum(np.exp(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42166536],\n",
       "       [-0.03602757],\n",
       "       [ 0.16906935],\n",
       "       [ 0.2182613 ],\n",
       "       [-0.54506423],\n",
       "       [ 0.16403491],\n",
       "       [-0.34890632],\n",
       "       [ 0.02628274],\n",
       "       [ 0.25615976],\n",
       "       [-0.38270639],\n",
       "       [ 0.06941713],\n",
       "       [ 0.22373832],\n",
       "       [-0.30081665],\n",
       "       [-0.37444577],\n",
       "       [-0.0119336 ],\n",
       "       [ 0.09038233],\n",
       "       [ 0.17227576],\n",
       "       [-0.24508553],\n",
       "       [ 0.33577998],\n",
       "       [-0.00172484]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = tanh(W1@x) #input to the first hidden layer after tanh activation\n",
    "h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53832149],\n",
       "       [0.        ],\n",
       "       [0.21715411],\n",
       "       [0.26443456],\n",
       "       [0.        ],\n",
       "       [0.25585572],\n",
       "       [0.1753095 ],\n",
       "       [0.00330432],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.04267007],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.35357084],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2 = ReLu(W2@h1) #input to the second hidden layer after ReLu activation\n",
    "h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output : y = \n",
      "[[0.41114087]\n",
      " [0.27802768]\n",
      " [0.31083144]]\n"
     ]
    }
   ],
   "source": [
    "y = SoftMax(W3@h2)\n",
    "print(\"Output : y = \\n{}\".format(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
